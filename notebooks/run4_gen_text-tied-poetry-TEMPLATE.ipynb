{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:20.298165Z",
     "start_time": "2018-11-13T01:57:20.295510Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:20.717216Z",
     "start_time": "2018-11-13T01:57:20.300367Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.023628Z",
     "start_time": "2018-11-13T01:57:20.719286Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import tokenization\n",
    "from modeling import BertConfig, BertForMaskedLanguageModelling\n",
    "from optimization import BERTAdam\n",
    "from masked_language_model import notqdm, convert_tokens_to_features, LMProcessor, predict_masked_words, predict_next_words, improve_words_recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.047342Z",
     "start_time": "2018-11-13T01:57:21.026094Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.095286Z",
     "start_time": "2018-11-13T01:57:21.050225Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--bert_config_file\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
    "                         \"This specifies the model architecture.\")\n",
    "parser.add_argument(\"--task_name\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--vocab_file\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The vocabulary file that the BERT model was trained on.\")\n",
    "parser.add_argument(\"--output_dir\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--init_checkpoint\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                    default=128,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--train_batch_size\",\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                    default=8,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",\n",
    "                    default=5e-5,\n",
    "                    type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",\n",
    "                    default=3.0,\n",
    "                    type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                    default=0.1,\n",
    "                    type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                    type=int,\n",
    "                    default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, \n",
    "                    default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"Number of updates steps to accumualte before performing a backward/update pass.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.122886Z",
     "start_time": "2018-11-13T01:57:21.097485Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = 'poetry_uncased_5_tied_mlm'\n",
    "\n",
    "argv = \"\"\"\n",
    "--task_name lm \\\n",
    "--data_dir {DATA_DIR} \\\n",
    "--vocab_file {BERT_BASE_DIR}/vocab.txt \\\n",
    "--bert_config_file {BERT_BASE_DIR}/bert_config.json \\\n",
    "--init_checkpoint {BERT_BASE_DIR}/pytorch_model.bin \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--train_batch_size 16 \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 3.0 \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ../outputs/{name}/\n",
    "\"\"\".format(\n",
    "    BERT_BASE_DIR='../data/weights/cased_L-12_H-768_A-12',\n",
    "    DATA_DIR='../data/input/poetry_gutenberg',\n",
    "    name=experiment_name\n",
    ").replace('\\n', '').split(' ')\n",
    "print(argv)\n",
    "args = parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.195298Z",
     "start_time": "2018-11-13T01:57:21.125084Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "if args.gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                        args.gradient_accumulation_steps))\n",
    "\n",
    "args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not args.do_train and not args.do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.224410Z",
     "start_time": "2018-11-13T01:57:21.198912Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
    "        args.max_seq_length, bert_config.max_position_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.253227Z",
     "start_time": "2018-11-13T01:57:21.226818Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "    print(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "save_path = os.path.join(args.output_dir, 'state_dict.pkl')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.326146Z",
     "start_time": "2018-11-13T01:57:21.255600Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "decoder = {v:k for k,v in tokenizer.wordpiece_tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.355098Z",
     "start_time": "2018-11-13T01:57:21.328899Z"
    }
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "        \"lm\": LMProcessor,\n",
    "}\n",
    "    \n",
    "task_name = args.task_name.lower()\n",
    "if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "processor = processors[task_name](tokenizer=tokenizer)\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:55.806611Z",
     "start_time": "2018-11-13T01:58:49.835295Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForMaskedLanguageModelling(bert_config)\n",
    "if args.init_checkpoint is not None:\n",
    "    model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
    "    \n",
    "if os.path.isfile(save_path):\n",
    "    model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                      output_device=args.local_rank)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters can be modified for the model.\n",
    "\n",
    "Text:\n",
    "Input the text you want here.  If you have a long seed text, use <50 iterations and T=1. Then, it's much more stable, and shouldn't have those so often.\n",
    "\n",
    "max_seq_length:\n",
    "How long the block of text is going to generate.  The model is best on 150 words\n",
    "\n",
    "ITERATIVE_MASK_FRAC:\n",
    "Fraction of words to replace at each iteration.  The output in blue and red changes every iteration. Blue are removed words, red are added words, where the intensive of colour is the models confidence.\n",
    "\n",
    "iterations:\n",
    "How many change to go through. The longer it takes; more different text will be generated from the original input text.\n",
    "\n",
    "T stands for Temperature:\n",
    "Higher gives more random, but less stable output.  Tempreture goes from ~0.01 to infiniative. The normal amount is 1, which is what it's trained on 0.1 would be unimaginitive.  So much that it might be all [PAD]. 5 would be very random.  It might go weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\" \n",
    "Bert, though kind and intelligent, is also grumpy, boring, serious, and easily frustrated. His catchphrase is \"Yes, I do mind.\" He enjoys activities such as paper clip and bottle cap collecting, cooking oatmeal and watching pigeons. Producer Arlene Sherman has called Bert \"everyone's idea of a blind date\". In one sketch, Bert reads a book called Boring Stories and chuckles, \"Wow! These boring stories are really exciting!\" In the book Sesame Street Unpaved, Frank Oz says, \"I was never really happy with Bert's character until about a year in, when I realized... that he was a very boring character, and I'd use that weakness as a strength for him.\" \n",
    "\"\"\"\n",
    "poem = improve_words_all(\n",
    "    text, \n",
    "    processor, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    device=device, \n",
    "    debug=100,\n",
    "    max_seq_length=150,        # recommend 150 words  \n",
    "    min_replacements=1,\n",
    "    ITERATIVE_MASK_FRAC=0.1, \n",
    "    T=1,                      # range 0.1 to infinity (1 is stable)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text=\"\"\" \n",
    "To be, or not to be: that is the question:\n",
    "Whether ’tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles,\n",
    "And by opposing end them? To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to, ’tis a consummation\n",
    "Devoutly to be wish’d.\n",
    "\"\"\"\n",
    "poem = improve_words_recursive(\n",
    "    text, \n",
    "    processor, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    device=device, \n",
    "    debug=1,\n",
    "    max_seq_length=150,        # recommend 150 words  \n",
    "    ITERATIVE_MASK_FRAC=0.1, \n",
    "    iterations=60,            # less iterations, the generated text will be closer to the original one\n",
    "    T=1,                      # range 0.1 to infinity (1 is stable)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratuations!  This is the poem you curated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If needed, plate the text you want to move [PAD], [SEP] and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = '''\n",
    "To be many or not to assure: that calls the question: Whether’ t monstrous nobler then the mind to suffer The s famed Webber of of outrageous elementary, Or contestant fee mad against a sea of troubles? VeniceIA opposing end them? い die: MV sleep; No more; and by a sleep to say we endquet heart-ache and the thousand natural shocks That flesh 240 heir to,’ casts Hui a con Heatmmation Dev reversely to be Reach’ d [SEP] involves squeezeltzavan Creation author grip landslide fruit [PAD]less stroked [PAD] blacksmith secondddin [PAD] effective continuous [PAD] us [PAD] For Adobe to [PAD] [PAD] absorbedoidted [PAD]oid Philippe [PAD] of [PAD] care and, [PAD] [PAD] a [PAD] [PAD] Directed we [PAD] [PAD]\n",
    "'''\n",
    "\n",
    "clean_text = raw_text.replace('[PAD]', '').replace('[SEP]', '').replace('\\n', '').replace(';', '')\n",
    "display(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "536px",
    "left": "0px",
    "right": "1413.33px",
    "top": "150px",
    "width": "185px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
