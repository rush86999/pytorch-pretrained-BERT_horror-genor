{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:20.298165Z",
     "start_time": "2018-11-13T01:57:20.295510Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:20.717216Z",
     "start_time": "2018-11-13T01:57:20.300367Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.023628Z",
     "start_time": "2018-11-13T01:57:20.719286Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import six\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "import tokenization\n",
    "from modeling import BertConfig, BertForMaskedLanguageModelling\n",
    "from optimization import BERTAdam\n",
    "from masked_language_model import notqdm, convert_tokens_to_features, LMProcessor, predict_masked_words, predict_next_words, improve_words_recursive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.047342Z",
     "start_time": "2018-11-13T01:57:21.026094Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T10:41:27.497266Z",
     "start_time": "2018-11-09T10:41:27.467141Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.095286Z",
     "start_time": "2018-11-13T01:57:21.050225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--gradient_accumulation_steps'], dest='gradient_accumulation_steps', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='Number of updates steps to accumualte before performing a backward/update pass.', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "## Required parameters\n",
    "parser.add_argument(\"--data_dir\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--bert_config_file\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The config json file corresponding to the pre-trained BERT model. \\n\"\n",
    "                         \"This specifies the model architecture.\")\n",
    "parser.add_argument(\"--task_name\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The name of the task to train.\")\n",
    "parser.add_argument(\"--vocab_file\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The vocabulary file that the BERT model was trained on.\")\n",
    "parser.add_argument(\"--output_dir\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    required=True,\n",
    "                    help=\"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "## Other parameters\n",
    "parser.add_argument(\"--init_checkpoint\",\n",
    "                    default=None,\n",
    "                    type=str,\n",
    "                    help=\"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "parser.add_argument(\"--do_lower_case\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to lower case the input text. True for uncased models, False for cased models.\")\n",
    "parser.add_argument(\"--max_seq_length\",\n",
    "                    default=128,\n",
    "                    type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                         \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                         \"than this will be padded.\")\n",
    "parser.add_argument(\"--do_train\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--train_batch_size\",\n",
    "                    default=32,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\",\n",
    "                    default=8,\n",
    "                    type=int,\n",
    "                    help=\"Total batch size for eval.\")\n",
    "parser.add_argument(\"--learning_rate\",\n",
    "                    default=5e-5,\n",
    "                    type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--num_train_epochs\",\n",
    "                    default=3.0,\n",
    "                    type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\"--warmup_proportion\",\n",
    "                    default=0.1,\n",
    "                    type=float,\n",
    "                    help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                         \"E.g., 0.1 = 10%% of training.\")\n",
    "parser.add_argument(\"--no_cuda\",\n",
    "                    default=False,\n",
    "                    action='store_true',\n",
    "                    help=\"Whether not to use CUDA when available\")\n",
    "parser.add_argument(\"--local_rank\",\n",
    "                    type=int,\n",
    "                    default=-1,\n",
    "                    help=\"local_rank for distributed training on gpus\")\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, \n",
    "                    default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument('--gradient_accumulation_steps',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help=\"Number of updates steps to accumualte before performing a backward/update pass.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.122886Z",
     "start_time": "2018-11-13T01:57:21.097485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--task_name', 'lm', '--data_dir', '../data/input/poetry_gutenberg', '--vocab_file', '../data/weights/cased_L-12_H-768_A-12/vocab.txt', '--bert_config_file', '../data/weights/cased_L-12_H-768_A-12/bert_config.json', '--init_checkpoint', '../data/weights/cased_L-12_H-768_A-12/pytorch_model.bin', '--do_train', '--do_eval', '--gradient_accumulation_steps', '2', '--train_batch_size', '16', '--learning_rate', '3e-5', '--num_train_epochs', '3.0', '--max_seq_length', '128', '--output_dir', '../outputs/poetry_uncased_5_tied_mlm/']\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'poetry_uncased_5_tied_mlm'\n",
    "\n",
    "argv = \"\"\"\n",
    "--task_name lm \\\n",
    "--data_dir {DATA_DIR} \\\n",
    "--vocab_file {BERT_BASE_DIR}/vocab.txt \\\n",
    "--bert_config_file {BERT_BASE_DIR}/bert_config.json \\\n",
    "--init_checkpoint {BERT_BASE_DIR}/pytorch_model.bin \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--train_batch_size 16 \\\n",
    "--learning_rate 3e-5 \\\n",
    "--num_train_epochs 3.0 \\\n",
    "--max_seq_length 128 \\\n",
    "--output_dir ../outputs/{name}/\n",
    "\"\"\".format(\n",
    "    BERT_BASE_DIR='../data/weights/cased_L-12_H-768_A-12',\n",
    "    DATA_DIR='../data/input/poetry_gutenberg',\n",
    "    name=experiment_name\n",
    ").replace('\\n', '').split(' ')\n",
    "print(argv)\n",
    "args = parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T15:26:30.278170Z",
     "start_time": "2018-11-09T15:26:30.248424Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.195298Z",
     "start_time": "2018-11-13T01:57:21.125084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/12/2018 22:31:25 - INFO - __main__ -   device cuda n_gpu 1 distributed training False\n"
     ]
    }
   ],
   "source": [
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device %s n_gpu %d distributed training %r\", device, n_gpu, bool(args.local_rank != -1))\n",
    "\n",
    "if args.gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                        args.gradient_accumulation_steps))\n",
    "\n",
    "args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "if not args.do_train and not args.do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.224410Z",
     "start_time": "2018-11-13T01:57:21.198912Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_config = BertConfig.from_json_file(args.bert_config_file)\n",
    "\n",
    "if args.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length {} because the BERT model was only trained up to sequence length {}\".format(\n",
    "        args.max_seq_length, bert_config.max_position_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.253227Z",
     "start_time": "2018-11-13T01:57:21.226818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory (../outputs/poetry_uncased_5_tied_mlm/) already exists and is not empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../outputs/poetry_uncased_5_tied_mlm/state_dict.pkl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "    print(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "save_path = os.path.join(args.output_dir, 'state_dict.pkl')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.326146Z",
     "start_time": "2018-11-13T01:57:21.255600Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=args.vocab_file, do_lower_case=args.do_lower_case)\n",
    "\n",
    "decoder = {v:k for k,v in tokenizer.wordpiece_tokenizer.vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:57:21.355098Z",
     "start_time": "2018-11-13T01:57:21.328899Z"
    }
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "        \"lm\": LMProcessor,\n",
    "}\n",
    "    \n",
    "task_name = args.task_name.lower()\n",
    "if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "processor = processors[task_name](tokenizer=tokenizer)\n",
    "label_list = processor.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:48.514022Z",
     "start_time": "2018-11-13T01:57:21.357795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f0c17571724f7d96fde8227a36f9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='tokenising', max=169371, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4da6b1b57972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m num_train_steps = int(\n\u001b[1;32m      3\u001b[0m     len(train_examples) / args.train_batch_size * args.num_train_epochs)\n",
      "\u001b[0;32m~/pytorch-pretrained-BERT_horror-genor/masked_language_model.py\u001b[0m in \u001b[0;36mget_train_examples\u001b[0;34m(self, data_dir, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m\"\"\"See base class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         return self._create_examples(\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-pretrained-BERT_horror-genor/masked_language_model.py\u001b[0m in \u001b[0;36m_create_examples\u001b[0;34m(self, lines, set_type, window_size, tqdm, skip)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokenising\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-pretrained-BERT_horror-genor/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-pretrained-BERT_horror-genor/tokenization.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_split_on_punc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhitespace_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch-pretrained-BERT_horror-genor/tokenization.py\u001b[0m in \u001b[0;36mwhitespace_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_examples = processor.get_train_examples(args.data_dir, skip=30, tqdm=tqdm)\n",
    "num_train_steps = int(\n",
    "    len(train_examples) / args.train_batch_size * args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T08:35:09.919396Z",
     "start_time": "2018-11-10T08:35:09.767935Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:49.803849Z",
     "start_time": "2018-11-13T01:58:48.516554Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_features = convert_tokens_to_features(\n",
    "    train_examples, label_list, args.max_seq_length, tokenizer, tqdm=tqdm)\n",
    "\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "all_label_weights = torch.tensor([f.label_weights for f in train_features], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T06:08:15.170422Z",
     "start_time": "2018-11-11T06:07:03.795167Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:49.833100Z",
     "start_time": "2018-11-13T01:58:49.806849Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids, all_label_weights)\n",
    "if args.local_rank == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T06:50:42.333283Z",
     "start_time": "2018-11-10T06:50:42.311379Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T07:13:01.317704Z",
     "start_time": "2018-11-10T07:13:00.880718Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:55.806611Z",
     "start_time": "2018-11-13T01:58:49.835295Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForMaskedLanguageModelling(bert_config)\n",
    "if args.init_checkpoint is not None:\n",
    "    model.bert.load_state_dict(torch.load(args.init_checkpoint, map_location='cpu'))\n",
    "    \n",
    "if os.path.isfile(save_path):\n",
    "    model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                      output_device=args.local_rank)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:55.835458Z",
     "start_time": "2018-11-13T01:58:55.809563Z"
    }
   },
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if n not in no_decay], 'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if n in no_decay], 'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "\n",
    "optimizer = BERTAdam(optimizer_parameters,\n",
    "                     lr=args.learning_rate,\n",
    "                     warmup=args.warmup_proportion,\n",
    "                t_total=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"What is more gentle than a wind in summer? What is more soothing than the pretty hummer That stays one moment in an open flower, And buzzes cheerily from bower to bower? What is more tranquil than a musk-rose blowing In a green island, far from all men's knowing?\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=20, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.397437Z",
     "start_time": "2018-11-13T01:58:55.837586Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# val_test=\"\"\"The next day I was somewhat somnolent, of which you may be sure Miss Frankland took no notice. She retired to her own room when we went for our recreation. My sisters scolded me for not coming to them the previous night, but I told them that Miss F. had continued to move about her room for so long a time that I had fallen fast asleep, and even then had not had enough, as they might have observed how sleepy I had been all\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=20, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.526350Z",
     "start_time": "2018-11-13T01:58:57.400186Z"
    }
   },
   "outputs": [],
   "source": [
    "display(predict_masked_words(val_test, processor, tokenizer, model, device=device, max_seq_length=args.max_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.549656Z",
     "start_time": "2018-11-13T01:58:57.528646Z"
    }
   },
   "outputs": [],
   "source": [
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.933014Z",
     "start_time": "2018-11-13T01:58:57.552204Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for _ in tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\"):\n",
    "    tr_loss, nb_tr_examples, nb_tr_steps = 0, 0, 0\n",
    "    with tqdm(total=len(train_dataloader), desc='Iteration', mininterval=60) as prog:\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids, label_weights = batch\n",
    "            loss, logits = model(input_ids, segment_ids, input_mask, label_ids, label_weights)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()    # We have accumulated enougth gradients\n",
    "                model.zero_grad()\n",
    "            prog.update(1)\n",
    "            prog.desc = 'Iter. loss={:2.6f}'.format(tr_loss/nb_tr_examples)\n",
    "            if step%3000==10:\n",
    "                \n",
    "                print('step', step, 'loss', tr_loss/nb_tr_examples)\n",
    "                display(predict_masked_words(val_test, processor, tokenizer, model, device=device, max_seq_length=args.max_seq_length))\n",
    "                display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=10, device=device))\n",
    "                tr_loss, nb_tr_examples, nb_tr_steps = 0, 0, 0\n",
    "                \n",
    "            # TODO validation test at end of each epoch to check for overfitting\n",
    "                \n",
    "    \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=10, device=device, debug=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.934071Z",
     "start_time": "2018-11-13T01:57:20.108Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.935435Z",
     "start_time": "2018-11-13T01:57:20.116Z"
    }
   },
   "outputs": [],
   "source": [
    "val_test=\"\"\"Frank could no longer resist\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=.1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.935435Z",
     "start_time": "2018-11-13T01:57:20.116Z"
    }
   },
   "outputs": [],
   "source": [
    "val_test=\"\"\"There was no doubt the lad had seen everything\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=26, T=.5, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T01:58:57.935435Z",
     "start_time": "2018-11-13T01:57:20.116Z"
    }
   },
   "outputs": [],
   "source": [
    "val_test=\"\"\"The next night I had been asleep about a couple of hours when I was suddenly awakened by\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"His mind spun in on itself\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=150, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"A giant spider descended on to\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=.5, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"Madness enveloped his mind as\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=.1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"A thin film of\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"Quivering with fear, he trembled as\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"Madness enveloped his mind as\"\"\"\n",
    "display(predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=300, T=1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"Roses are red, violets are blue, you are something but something\"\"\"\n",
    "predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=100, T=1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"The sun shone on the great home \"\"\"\n",
    "h=predict_next_words(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, n=12, T=1, device=device)\n",
    "h.data = h.data.replace('.', '.<p></p>')\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_words_recursive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"Quivering with fear, he trembled, he, but lo! Fell he, Again until emptied of life, Again; Till, when Love sighed-- Love dreamed, but dreaming his; and the still stream he very much esteems I plac, or lie large in size Balyms others attributed to Dashdims-- saint of Bat. 4. 143 Bb version-Morgen 144 3 5 sing the Heathings 48 8 singbi 79 12 13 5 4 5 6 Vuivering with fear, he trembled, he, but lo! Fell he, Again until emptied of life, Again; Till, when Love sighed-- Love dreamed, but dreaming his; and the still stream he very much esteems I plac, or lie large in size Balyms others attributed to Dashdims-- saint of Bat. 4. 143 Bb version-Morgen 144 3 5 sing the Heathings\"\"\"\n",
    "display(improve_words_recursive(\n",
    "    val_test, \n",
    "    processor, \n",
    "    tokenizer, \n",
    "    model, \n",
    "    max_seq_length=args.max_seq_length, \n",
    "    iterations=50, # How many change to go through, it replaces \n",
    "    T=1, # Tempreture - Higher gives more random, but less stable output\n",
    "    device=\"cuda\", \n",
    "    debug=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test=\"\"\"What is more gentle than a wind in summer. How is more hearty than a winters soul. Why are we glum unto the Autumn. And where are we that we may skip in the spring. Unto thee I ask the question of the seasons, so you furnish me with empty lies and we may hunt over all for the final lie so we my sleep\"\"\"\n",
    "display(improve_words_recursive(val_test, processor, tokenizer, model, max_seq_length=args.max_seq_length, iterations=50, T=1, device=\"cuda\", debug=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"It's strange to think that we will live and die\n",
    "in the eternal grey,\n",
    "After years of the same things; sin and toil\n",
    "and empty delight.\n",
    "I will wed, then father and perish\n",
    "what else can I do?\n",
    "I bore as did those who came before me\n",
    "without question or blame.\n",
    "And go hither unto oblivion, leaving neither trace nor name\n",
    "\"\"\"\n",
    "improve_words_recursive(text, processor, tokenizer, model, iterations=100, max_seq_length=128, n=10, T=0.9, device=\"cuda\", debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"Mary had a little lamb\n",
    "Little lamb, little lamb\n",
    "Mary had a little lamb\n",
    "Its fleece was white as snow\n",
    "And everywhere that Mary went\n",
    "Mary went, Mary went\n",
    "Everywhere that Mary went\n",
    "The lamb was sure to go\n",
    "\n",
    "He followed her to school one day\n",
    "School one day, school one day\n",
    "He followed her to school one day\n",
    "Which was against the rule\n",
    "It made the children laugh and play\n",
    "Laugh and play, laugh and play\n",
    "It made the children laugh and play\n",
    "To see a lamb at school\"\"\"\n",
    "improve_words_recursive(text, processor, tokenizer, model, ITERATIVE_MASK_FRAC=0.1, iterations=100, max_seq_length=128, n=10, T=0.3, device=\"cuda\", debug=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"The rose is red, the violet's blue,\n",
    "    The honey's sweet, and so are you.\n",
    "    Thou are my love and I am thine;\n",
    "    I drew thee to my Valentine:\n",
    "    The lot was cast and then I drew,\n",
    "    And Fortune said it shou'd be you. The rose is red, the violet's blue,\n",
    "    The honey's sweet, and so are you.\n",
    "    Thou are my love and I am thine;\n",
    "    I drew thee to my Valentine:\n",
    "    The lot was cast and then I drew,\n",
    "    And Fortune said it shou'd be you.\"\"\"\n",
    "improve_words_recursive(text, processor, tokenizer, model, ITERATIVE_MASK_FRAC=0.2, iterations=100, max_seq_length=128, T=0.7, device=\"cuda\", debug=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"I saw the best minds of my generation destroyed by madness, starving hysterical naked,\n",
    "\n",
    "dragging themselves through the negro streets at dawn looking for an angry fix,\n",
    "\n",
    "angelheaded hipsters burning for the ancient heavenly connection to the starry dynamo in the machinery of night,\n",
    "\n",
    "who poverty and tatters and hollow-eyed and high sat up smoking in the supernatural darkness of cold-water flats floating across the tops of cities contemplating jazz,\n",
    "\n",
    "who bared their brains to Heaven under the El and saw Mohammedan angels staggering on tenement roofs illuminated,\n",
    "\n",
    "who passed through universities with radiant eyes hallucinating Arkansas and Blake-light tragedy among the scholars of war,\n",
    "\n",
    "who were expelled from the academies for crazy & publishing obscene odes on the windows of the skull,\n",
    "\n",
    "who cowered in unshaven rooms in underwear, burning their money in wastebaskets and listening to the Terror through the wall,\n",
    "\n",
    "who got busted in their pubic beards returning through Laredo with a belt of marijuana for New York,\n",
    "\n",
    "who ate fire in paint hotels or drank turpentine in Paradise Alley, death, or purgatoried their torsos night after night\n",
    "\n",
    "with dreams, with drugs, with waking nightmares, alcohol and cock and endless balls,\n",
    "\n",
    "incomparable blind streets of shuddering cloud and lightning in the mind leaping towards poles of Canada & Paterson, illuminating all the motionless world of Time between,\n",
    "\n",
    "Peyote solidities of halls, backyard green tree cemetery dawns, wine drunkenness over the rooftops, storefront boroughs of teahead joyride neon blinking traffic light, sun and moon and tree vibrations in the roaring winter dusks of Brooklyn, ashcan rantings and kind king light of mind,\"\"\"\n",
    "improve_words_recursive(text, processor, tokenizer, model, ITERATIVE_MASK_FRAC=0.2, iterations=100, max_seq_length=128, n=10, T=1, device=\"cuda\", debug=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as, and the sky's blue, And love's sea, and I was love; And and I of love: I was bide: Love gave me to the sea: Love gave me, and I was sea, And ay it was Fortune's betead. The sky is blue, and man's sea, The sea of the star, and of the sea; And I was deep, though I sing it; And I did say of the sea. And and and and and and I said; And then and then, of the hills and the sea, and and and,\n",
    "\n",
    "And stan'd down on the sea. But ere the the, while they slept, He left them, give'd them upon the dew, He lay'd their bodies in the dew. On the shore were the boars'slings in the dew, Roasting o'er all the hot dew; He rose, and gave them the dew. He bore them down to the sea, He lay'd them with them, On the spray of the and water. Twowain and twas, He led their heads to the shore, And bounding\n",
    "\n",
    "reason's it I think that we will live and engage in the same works, The works of the same, with fire and toil and empty heart. I was a woman then, but now for what else can I hope. We talk nothing, all that I go under to do... and achieve at all some oar, but do know the one and, to, do have, I after know. things I and that of, of can. I another. do or also over of, spring, have of, little secure, a-and at, at work of one yours,, heart and I\n",
    "\n",
    "quivering with fear, But he stood there, and ah! saw him, he was void of sight, O!-- and he prayed-- he prayed, and raised his hand to the holy book to see, Too little a mythismen or dream: and the psalm Of living things do to you, my saintly friend. VI. \" Bless ye the holyrysm-- and the book he held him,-- He saw it, \" ( quivering with fear ) He saw, he prayed, lo! but he lived, was void of life, O; he\n",
    "\n",
    "carries me out, for one good in each. He is more hearty than the single thing. They are under the darkness of the heaven. And, am I, I may go down the roads, tiptoed off, up out of the mountains. Never you will lead me with a child and we may hold it over me or you or over-night me all night crying night of and and it, me, any, you and and-everywhere and and me in all you a you night, We. you and the me under together up still and and of the thing thing.. up all we ahead andto\n",
    "\n",
    "it's enough to talk with spirits that see and know in the eternal things of the great, the great, that love to toil and toil. I am one of their friends and foes, and am I one? I am as a one who had it. I am the voice, that passes over to the turner, that is the voice of the life one that it of, the the of,, it is me the man it all, the you., the to living the, of of voice and the the the, to is and, the the one, the great, that love, to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- show probability in next word logging. Record probability of each letter, then use them when displaying as html\n",
    "- try other ways of doing next word. E.g. going back and redoing, doing more than 1 at once\n",
    "- make the masked language generator often mask last word\n",
    "- should I be doing loss on just the masked words, or all? It's hard to tell from the tensorflow repo. This is marked with a TODO or FIXME in the code\n",
    "- add validation loss, since overfitting seems to be a factor\n",
    "- for eval, don't pad, just have a batch size of one. That may lead to better results\n",
    "- for eval, add some words, and let it fill in the blanks\n",
    "  - recursivly replace low confidence words?\n",
    "  - for this I may have to make it predict unmasked words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for a result in the training text \n",
    "\n",
    "To check it's not just remembering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = open(os.path.join(args.data_dir,'train.txt')).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib \n",
    "candidate_test = 'of the morning of the morning and the evening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for cand_train in tqdm(train_txt):\n",
    "    diffl  = difflib.SequenceMatcher(None, cand_train.lower().strip(), candidate_test.lower() ).ratio()\n",
    "    matches.append([diffl, cand_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(matches, columns=['ratio', 'str'])\n",
    "df = df.sort_values('ratio', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(matches, columns=['ratio', 'str'])\n",
    "df = df.sort_values('ratio', ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_p36]",
   "language": "python",
   "name": "conda-env-pytorch_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "5",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "103px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "536px",
    "left": "0px",
    "right": "1413.33px",
    "top": "150px",
    "width": "185px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
